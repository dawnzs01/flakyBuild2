# V3.2.1.0 版本更新日志

- [ 新增 ] 文件分发的 `保险模式` 增加带日期的目录名选项，为文件备份需求场景提供更好的支持
- [ 新增 ] 扩展 `命名空间` 变量的能力，作业流程现在支持获取多主机的汇聚信息
  > 1. 当前仅支持在 bash 脚本内对 `命名空间` 变量进行赋值并使用变量值汇聚
  > 2. 获取的变量汇聚值为 `json` 字符串格式
  > 3. 需要在脚本中声明要获取全部或某个命名空间变量的数据汇聚（减少因其他不需要的场景产生不必要的开销，提升引擎的处理效率）

使用方式示例：

```bash
  #!/bin/bash
  
  # job_import {{JOB_NAMESPACE_ALL}}					　-- 获取所有命名空间变量的汇聚值（必须！）
  # job_import {{JOB_NAMESPACE_命名空间变量名}}	  -- 获取某个命名空间变量的汇聚值（必须！）
  
  echo ${JOB_NAMESPACE_ALL}
  echo ${JOB_NAMESPACE_命名空间变量名}
```

输出结果示例：

```bash
  ### echo ${JOB_NAMESPACE_ALL} 的输出（namespace_var1 和 namespace_var2 分别表示具体的某2个命名空间变量名）：
  {"namespace_var1":{"0:10.10.10.1":"xxxx","0:10.10.10.2":"yyyy","0:10.10.10.3":"zzzz"},"namespace_var2":{"0:20.20.20.1":"aaaa","0:20.20.20.2":"bbbb","0:20.20.20.3":"cccc","0:20.20.20.4":"dddd"}}
  
  ### echo ${JOB_NAMESPACE_命名空间变量名} 的输出：
  {"0:10.10.10.1":"xxxx","0:10.10.10.2":"yyyy","0:10.10.10.3":"zzzz"}
```

Shell 脚本处理 JSON字符串的两种方式（仅供参考，正式投入使用建议调试确认后再上线）：

方式一：（纯 sed + awk 实现）

```bash
  #!/bin/bash
  
  str='{"ns1":{"ip1":"value1","ip2":"value2"},"ns2":{"ip3":"value3","ip4":"value4"}}'
  
  function json_parse {
      local _json=$1
      local _key=$2
      temp=`echo $_json | sed 's/\\\\\//\//g' | sed 's/[{}]//g' | awk -v k="text" '{n=split($0,a,","); for (i=1; i<=n; i++) print a[i]}' | sed 's/\"\:\"/\|/g' | sed 's/[\,]/ /g' | sed 's/\"//g' | grep -w $_key`
      echo ${temp##*|}
  }
  
  json_parse $str ip1			## 获取 str 的 json字符串中 ip1 的值，即：value1
  json_parse $str ns1			## 获取 str 的 json字符串中 ns1 的值，即：{"ip1":"value1","ip2":"value2"}
```

方式二：（借助 Python 的 json 模块实现）

```bash
  #!/bin/bash
  
  str='{"ns1":{"ip1":"value1","ip2":"value2"},"ns2":{"ip3":"value3","ip4":"value4"}}'
  
  ### 获取 str 的 json字符串中 ip2 的值，即：value2
  echo ${str} | python -c 'import json,sys;obj=json.load(sys.stdin);print obj["ns1"]["ip2"]'
```

- [ 新增 ] 运营视图功能正式上线！
	提供更直观、细节的数据运营能力，帮助平台管理员更好的进行日常运营和分析工作



- [ 优化 ] 对步骤重试后的次数切换查看进行更直观的视觉体现增强
- [ 优化 ] 后台重新优化日志数据结构和存储的索引，提升对超大日志场景的性能
- [ 优化 ] 调整大批量执行任务的结果查询轮训逻辑，增强后台系统保护机制
- [ 优化 ] 一些常规的 UI / UX 优化...



- [ 修复 ] 解决多主机批次+大日志量的日志导出失败问题
- [ 修复 ] 解决任务重试后，执行日志没有正常刷新的问题
- [ 修复 ] 调整后台日志数据结构和处理逻辑，解决在任务日志量大的场景下查询会卡住的问题